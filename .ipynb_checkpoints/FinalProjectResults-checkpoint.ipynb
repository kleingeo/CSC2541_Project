{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation with cGANs for Improved Prostate Segmentation\n",
    "## Abstract \n",
    "\n",
    "Segmentation of medical images has many diverse applications, ranging from surgical planning to disease diagnosis. Machine learning approaches to segmentation using deep convolutional neural networks (CNNs) have demonstrated state-of-the-art results compared to the current gold-standard of manual segmentation. However, CNNs require large amounts of labeled training data and it is often impractical or impossible to obtain a sufficient number of medical images to successfully develop segmentation models. We present a novel method of data augmentation to increase the size of the labeled training set, involving a conditional generative adversarial network (cGAN) which is used to generate labeled synthetic data. First, a UNet is trained on the segmentation task using the available data. Then, segmentations corresponding to an anatomical atlas are fed back through a new UNet to generate synthetic data. This backward fed UNet is used as the generator of the cGAN, which has in-effect been pre-trained using the reverse segmentation task. The addition of the discriminator fine-tunes the output to encourage realistic synthetic data which corresponds to the conditioned segmentation mask (i.e result is a labeled training example). Our model is demonstrated on the Medical Segmentation Decathlon Prostate dataset consisting of 32 T2 weighted MRI volumes and shows improved segmentation performance compared to the non-augmented dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Establishing Baseline UNet \n",
    "\n",
    "The first thing is to establish a baseline that can be used to compare the exxperimental model to. This is \n",
    "used using three codes in combination; a Data generator, a Trainer, and a Predictor. \n",
    "\n",
    "### Data Generator \n",
    "Requirements: \n",
    "\n",
    "    data_dir: directory of where training data is saved. MRI data should be in the format of nii.gz\n",
    "    target_dir: corresponding segmentation files for each example in the data_dir. File format: nii.gz\n",
    "    batch_size: batch size\n",
    "    shuffle: choice if data should be reshhuffled between epochs. Boolea\n",
    "    num_channels: number of channels for the input shape\n",
    "    num_classes: number of class labels for the output shape\n",
    "    input_size: size of the image input\n",
    "    regular: direction that the network is being trained, True means that the input is an image and the output is a segmentation, False means the reverse\n",
    "   \n",
    "### Trainer\n",
    "Requirements: \n",
    "\n",
    "    batch_folder_train: directory of where training data is saved. MRI data should be in the format of nii.gz\n",
    "    target_folder_train: corresponding segmentation files for each example in the data_dir. File format: nii.gz\n",
    "    ofolder: output folder to save the model weights and json files in \n",
    "    samples_per_card=None: Used for multi-gpu training\n",
    "    epochs=50: number of epochs during training\n",
    "    batch_size=50: batch size given to the Data Generator\n",
    "    gpus_used=1: amount off GPUs available for training \n",
    "    training_direction=True: Direction of training. See 'regular' in Data Generator\n",
    "    num_classes=1: number of class labels for the output shape\n",
    "    train_aug=False: choice of using a generator to augment the data \n",
    "    aug_folder=None: The folder the holds the weights and json file for the generator to create synthetic data\n",
    "    batch_folder_val=None: folder of testing/validation set. File Format: nii.gz\n",
    "    target_folder_val=None: corresponding target values for the testing/validation set. File Format: nii.gz\n",
    "    num_syn_data=None: Choice of how many synthetic data examples are used. \n",
    "    \n",
    "### Predictor \n",
    "Requirements: \n",
    "\n",
    "    model_folder: Folder that contains the model weights and json file you are looking to test\n",
    "    data_folder: Folder of the testing/validation data you are testing on. File Format: nii.gz \n",
    "    target_folder: Folder of the corresponding targets for the testing/validation data. File Format: nii.gz\n",
    "    ofolder: Output folder where results are saved. Result numpy array of segmentation probability map and a csv of DSC calculated for the probability mask thersholded above 0.5. \n",
    "    opt: Optimizer that was used for traing. \n",
    "    testing_direction=True: direction of testing. See regular in Data Generator. \n",
    "\n",
    "### Data Set \n",
    "The data set can be downloaded from this URL: https://drive.google.com/open?id=1Ff7c21UksxyT4JfETjaarmuKEjdqe1-a\n",
    "Then take training data and split into testing and training folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Trainer import Trainer\n",
    "from Predictor import Predictor\n",
    "import os\n",
    "import keras as K\n",
    "\n",
    "'''This script is used to train a basic UNet on prostate segmentation in\n",
    "MRI. This will be used as a base line in our project. It is traained on\n",
    "75% of the volumes, resampled to istropic voxel size of 2.0. It is then tested on 25% of the volumes,\n",
    "resampled the same, resulting in 278 testing examples.\n",
    "'''\n",
    "\n",
    "### Train the Model\n",
    "\n",
    "# Change these folders to your desired directories.\n",
    "data_dir_train = '/prostate_data/Task05_Prostate/imagesTr/'\n",
    "\n",
    "target_dir_train = '/prostate_data/Task05_Prostate/labelsTr/'\n",
    "\n",
    "data_dir_val = '/prostate_data/Task05_Prostate/imagesTs/'\n",
    "\n",
    "target_dir_val = '/prostate_data/Task05_Prostate/labelsTs/'\n",
    "\n",
    "ofolder = 'ModelOutputs/UNet_regular_rev2'\n",
    "\n",
    "if 'CUDA_VISIBLE_DEVICES' in os.environ.keys():\n",
    "    CUDA_VISIBLE_DEVICES = os.environ['CUDA_VISIBLE_DEVICES'].split(',')\n",
    "else:\n",
    "    CUDA_VISIBLE_DEVICES = ['None']\n",
    "\n",
    "a = Trainer(data_dir_train, target_dir_train, ofolder, samples_per_card=None,\n",
    "            epochs=50, gpus_used=len(CUDA_VISIBLE_DEVICES), num_classes=1,\n",
    "            batch_size=16, training_direction=True,\n",
    "            batch_folder_val=data_dir_val, target_folder_val=target_dir_val\n",
    "            )\n",
    "\n",
    "a.train_the_model(t_opt=K.optimizers.adam(lr=1e-5))\n",
    "\n",
    "### Test the Model\n",
    "# Change these folders to your desired directories.\n",
    "data_dir_val = '/prostate_data/Task05_Prostate/imagesTs\n",
    "\n",
    "target_dir_val = '/prostate_data/Task05_Prostate/labelsTs/'\n",
    "\n",
    "model_folder = 'ModelOutputs/UNet_regular_rev2'\n",
    "\n",
    "ofolder = 'ModelOutputs/UNet_regular_rev2/test_results'\n",
    "\n",
    "a = Predictor(model_folder=model_folder,\n",
    "              data_folder=data_dir_val,\n",
    "              target_folder=target_dir_val,\n",
    "              ofolder=ofolder,\n",
    "              opt='ADAM',\n",
    "              testing_direction=True)\n",
    "a.predict_and_evaluate()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-Training the UNet Generrator \n",
    "For the next step we turn the training in reverse to pre-train weights to generate images given the segmentation ground truth. This just sets the training_direction to False and runs the trainer again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Trainer import Trainer\n",
    "import os\n",
    "import keras as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Train the Augmentation Model\n",
    "\n",
    "data_dir = '/prostate_data/Task05_Prostate/imagesTr/'\n",
    "\n",
    "target_dir = '/prostate_data/Task05_Prostate/labelsTr/'\n",
    "\n",
    "ofolder = 'ModelOutputs/UNetAugmentor_rev3/'\n",
    "\n",
    "\n",
    "if 'CUDA_VISIBLE_DEVICES' in os.environ.keys():\n",
    "    CUDA_VISIBLE_DEVICES = os.environ['CUDA_VISIBLE_DEVICES'].split(',')\n",
    "else:\n",
    "    CUDA_VISIBLE_DEVICES = ['None']\n",
    "\n",
    "\n",
    "a = Trainer(data_dir, target_dir, ofolder, samples_per_card=None,\n",
    "            epochs=100, gpus_used=len(CUDA_VISIBLE_DEVICES),\n",
    "            batch_size=16, training_direction=False)\n",
    "\n",
    "a.train_the_model(t_opt=K.optimizers.adam(lr=1e-4),\n",
    "                  loss=K.losses.mae,\n",
    "                  t_depth=4,\n",
    "                  t_dropout=0.5)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training and Testing a UNet using the UNet Generator \n",
    "This is an optional step. This can be ran to show that the pre-trained weights have found anything helpful for augmenting the data. The process is the same as Step 1, only setting train_aug=True and giving an aug_folder for the weights and json file of the generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Trainer import Trainer\n",
    "from Predictor import Predictor\n",
    "import os\n",
    "import keras as K\n",
    "\n",
    "# Train the Model\n",
    "\n",
    "data_dir = '/prostate_data/Task05_Prostate' \\\n",
    "           '/imagesTr/'\n",
    "target_dir = '/prostate_data/Task05_Prostate' \\\n",
    "             '/labelsTr/'\n",
    "ofolder = '/2019-03-30-CSC2541Project/UNet_regularWAug/'\n",
    "\n",
    "aug_folder = '/2019-03-30-CSC2541Project/UNetAugmentor/'\n",
    "\n",
    "if 'CUDA_VISIBLE_DEVICES' in os.environ.keys():\n",
    "    CUDA_VISIBLE_DEVICES = os.environ['CUDA_VISIBLE_DEVICES'].split(',')\n",
    "else:\n",
    "    CUDA_VISIBLE_DEVICES = ['None']\n",
    "\n",
    "a = Trainer(data_dir, target_dir, ofolder, samples_per_card=10,\n",
    "            epochs=50, gpus_used=len(CUDA_VISIBLE_DEVICES),\n",
    "            batch_size=None, training_direction=True,\n",
    "            data_aug=True,\n",
    "            aug_folder=aug_folder)\n",
    "\n",
    "a.train_the_model(t_opt=K.optimizers.adam(lr=1e-5))\n",
    "\n",
    "# Test the Model\n",
    "\n",
    "data_dir = '/prostate_data/Task05_Prostate' \\\n",
    "           '/imagesTs/'\n",
    "target_dir = '/prostate_data/Task05_Prostate' \\\n",
    "             '/labelsTs/'\n",
    "model_folder = '/2019-03-30-CSC2541Project/UNet_regularWAug/'\n",
    "\n",
    "ofolder = '/2019-03-30-CSC2541Project/UNet_regularWAug' \\\n",
    "          '/test_results'\n",
    "\n",
    "a = Predictor(model_folder=model_folder,\n",
    "              data_folder=data_dir,\n",
    "              target_folder=target_dir,\n",
    "              ofolder=ofolder,\n",
    "              opt='ADAM',\n",
    "              testing_direction=True)\n",
    "a.predict_and_evaluate()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fine Tuning the Unet Generator in a GANs training Framework \n",
    "This script used an adaptation of a pix2pix.py code published at https://github.com/eriklindernoren/Keras-GAN \n",
    "\n",
    "### Pix2Pix \n",
    "Requirements: \n",
    "\n",
    "    data_dir: directory where the training data is located. File Format nii.gz\n",
    "    target_dir: directory where the corresponding target masks are found for the training data. File Format: nii.gz \n",
    "    batch_size: batch size to be used in training\n",
    "    pretrained_folder: folder where the pre-trained model is saved \n",
    "    ofolder: Output folder. Will create png files for each epoch, save weights and a json file \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Pix2Pix import Pix2Pix\n",
    "import keras as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Train the Augmentation Model using a cGAN\n",
    "\n",
    "data_dir = '/prostate_data/Task05_Prostate' \\\n",
    "           '/imagesTr/'\n",
    "target_dir = '/prostate_data/Task05_Prostate' \\\n",
    "             '/labelsTr/'\n",
    "pretrained_model = 'ModelOutputs/UNetAugmentor_rev3/'\n",
    "ofolder = 'ModelOutputs/cGANUnetAugmentor_rev2/'\n",
    "\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "cgan = Pix2Pix(pretrained_folder=pretrained_model,\n",
    "               data_dir=data_dir,\n",
    "               target_dir=target_dir,\n",
    "               ofolder=ofolder,\n",
    "               batch_size=32)\n",
    "\n",
    "cgan.train(epochs=2000, sample_interval=1000)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training and Testing using the cGAN trained UNet Generator \n",
    "Again, this is a repeat of step 1 and 3, just with setting data_aug=True and giving a aug_folder for the UNet generator. In this experiment we also ran training adn testing with different portions of synthetic data used in the training set, as per the results shown in the final report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Trainer import Trainer\n",
    "from Predictor import Predictor\n",
    "import os\n",
    "import keras as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "\n",
    "data_dir_train = '/prostate_data/Task05_Prostate' \\\n",
    "           '/imagesTr/'\n",
    "target_dir_train = '/prostate_data/Task05_Prostate' \\\n",
    "             '/labelsTr/'\n",
    "\n",
    "data_dir_val = '/prostate_data/Task05_Prostate' \\\n",
    "           '/imagesTs/'\n",
    "target_dir_val = '/prostate_data/Task05_Prostate' \\\n",
    "             '/labelsTs/'\n",
    "\n",
    "aug_folder = 'ModelOutputs/cGANUnetAugmentor_rev2/'\n",
    "\n",
    "num_syn_data_vec = np.linspace(100, 2000, 20)\n",
    "\n",
    "for num_syn_data in num_syn_data_vec:\n",
    "\n",
    "    ofolder = 'ModelOutputs/UNet_reuglarWAugcGAN_rev2/{}_syn_samples/'.format(int(num_syn_data))\n",
    "\n",
    "    if os.path.exists(ofolder) is False:\n",
    "\n",
    "        os.makedirs(ofolder)\n",
    "\n",
    "    if 'CUDA_VISIBLE_DEVICES' in os.environ.keys():\n",
    "        CUDA_VISIBLE_DEVICES = os.environ['CUDA_VISIBLE_DEVICES'].split(',')\n",
    "    else:\n",
    "        CUDA_VISIBLE_DEVICES = ['None']\n",
    "\n",
    "    a = Trainer(data_dir_train, target_dir_train, ofolder, samples_per_card=None,\n",
    "                epochs=50, gpus_used=len(CUDA_VISIBLE_DEVICES), num_classes=1,\n",
    "                batch_size=16, training_direction=True,\n",
    "                aug_folder=aug_folder,\n",
    "                data_aug=True, num_syn_data=int(num_syn_data),\n",
    "                batch_folder_val=data_dir_val, target_folder_val=target_dir_val\n",
    "                )\n",
    "\n",
    "    a.train_the_model(t_opt=K.optimizers.adam(lr=1e-5))\n",
    "\n",
    "    # Test the Model\n",
    "\n",
    "    data_dir = '/jaylabs/amartel_data2/prostate_data/Task05_Prostate' \\\n",
    "               '/imagesTs/'\n",
    "    target_dir = '/jaylabs/amartel_data2/prostate_data/Task05_Prostate' \\\n",
    "                 '/labelsTs/'\n",
    "    model_folder = ofolder\n",
    "\n",
    "    ofolder_pred = ofolder + 'test_results'\n",
    "\n",
    "    a = Predictor(model_folder=model_folder,\n",
    "                  data_folder=data_dir,\n",
    "                  target_folder=target_dir,\n",
    "                  ofolder=ofolder_pred,\n",
    "                  opt='ADAM',\n",
    "                  testing_direction=True)\n",
    "    a.predict_and_evaluate()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization \n",
    "\n",
    "For the final results visualization was done using 2 codes in the repositorry \n",
    "\n",
    "### plot_images.py \n",
    "This script needs manual editing and given the directory for the model you want to build visual examples for, as well as the testing folders for data and targets. \n",
    "\n",
    "### plot_hisotgram\n",
    "This script also requries manual editing. Given a csv file, it will plot a histogram of the DSC for each test subject in the csv database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
